import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# Generate synthetic dataset
np.random.seed(42)
n_samples = 10000

# Generate features
passenger_count = np.random.randint(1, 6, n_samples)
distance = np.random.uniform(0.5, 50, n_samples)  # in km

# Generate fare_amount based on distance and passenger_count with some noise
base_fare = 2.5
per_km = 1.5
fare_amount = base_fare + per_km * distance + np.random.normal(0, 2, n_samples)
fare_amount = np.maximum(fare_amount, 0)  # ensure positive fares

# Create DataFrame
df = pd.DataFrame({
    'passenger_count': passenger_count,
    'distance': distance,
    'fare_amount': fare_amount
})

# Display initial info
print("Initial dataset shape:", df.shape)
print(df.head())
print(df.info())

# 1. Pre-process the dataset
# Drop rows with missing values
df.dropna(inplace=True)

# Filter out invalid data (e.g., negative fares, zero distance, etc.)
df = df[(df['fare_amount'] > 0) & (df['distance'] > 0) & (df['passenger_count'] > 0)]

# Select relevant features
features = ['passenger_count', 'distance']
target = 'fare_amount'
X = df[features]
y = df[target]

print("After preprocessing shape:", df.shape)

# 2. Identify outliers
# Use IQR to detect outliers in fare_amount
Q1 = y.quantile(0.25)
Q3 = y.quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = (y < lower_bound) | (y > upper_bound)
print(f"Number of outliers in fare_amount: {outliers.sum()}")

# Remove outliers
df_no_outliers = df[~outliers]
X = df_no_outliers[features]
y = df_no_outliers[target]

print("After removing outliers shape:", df_no_outliers.shape)

# 3. Check the correlation
correlation_matrix = df_no_outliers[features + [target]].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# 4. Implement models
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

# Random Forest Regression
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# 5. Evaluate the models
def evaluate_model(y_true, y_pred, model_name):
    r2 = r2_score(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    print(f"{model_name} - R2: {r2:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}")
    return r2, rmse, mae

print("\nModel Evaluation:")
lr_scores = evaluate_model(y_test, y_pred_lr, "Linear Regression")
rf_scores = evaluate_model(y_test, y_pred_rf, "Random Forest Regression")

# Compare
if rf_scores[0] > lr_scores[0]:
    print("Random Forest performs better in terms of R2.")
else:
    print("Linear Regression performs better in terms of R2.")
